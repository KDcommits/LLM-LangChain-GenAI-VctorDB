{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_MryiiPNR6z"
      },
      "source": [
        "<h1 align='center'> <strong>LangChain </strong></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LByfFa3kHavl"
      },
      "source": [
        "#### Setting up the env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zYxw_3EHRw4",
        "outputId": "21ff9131-22b5-4701-b27f-7388512ed64a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.1/179.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain -q\n",
        "!pip install --upgrade openai -q\n",
        "!pip install huggingface_hub -q\n",
        "!pip install openai -q\n",
        "!pip install python-dotenv -q\n",
        "!pip install pinecone-client -q\n",
        "!pip install -U sentence-transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yjR6MFsHUD0",
        "outputId": "fe0383fd-3dcf-4697-b8b7-f7807ae02eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "### Mount to g-drive (Sign in required for the mount)\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DCjGIBC_HVt9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from dotenv import load_dotenv\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX035NIiHYJ5",
        "outputId": "084fc425-821a-4de5-9681-ac64c6db9087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Python_DA_DS_ML/Gen AI/LLM & Langchain\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%cd '/content/gdrive/MyDrive/Python_DA_DS_ML/Gen AI/LLM & Langchain'\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORuwLuajHf0E"
      },
      "source": [
        "#### Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dSpFnjyHdLl"
      },
      "source": [
        "**LangChain**, a framework for building applications powered by large language models (LLMs).<br>\n",
        "It provides modules for managing and optimizing the use of language models in applications. Its core philosophy is to facilitate data-aware applications where the language model interacts with other data sources(pdf/excel/web) and its environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB-s5QIBfhZv"
      },
      "source": [
        "The core idea of the library is that we can “chain” together different components to create more advanced use cases around LLMs. Chains may consist of multiple components from several modules:<br>\n",
        "\n",
        "- **Prompt templates:** Prompt templates are templates for different types of prompts. Like “chatbot” style templates, ELI5 question-answering, etc\n",
        "\n",
        "- **LLMs:** Large language models like GPT-3, BLOOM, etc\n",
        "\n",
        "- **Agents:** Agents use LLMs to decide what actions should be taken. Tools like web search or calculators can be used, and all are packaged into a logical loop of operations.\n",
        "\n",
        "- **Memory:** Short-term memory, long-term memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaLl8JEUfysJ"
      },
      "source": [
        "### Prompt Template :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaE3hrqYf2cv"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(\n",
        "        template=template,\n",
        "        input_variables=['question']\n",
        ")\n",
        "\n",
        "# user question\n",
        "question = \"Which team won the 2011 Cricket world cup\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu5z97IMi3Gl"
      },
      "source": [
        "### LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzsbOkHDikOa"
      },
      "source": [
        "##### Option 1: LLM from HF Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMGoPzcFU8_E"
      },
      "source": [
        "Usefullness of HuggingFace Hub: <br>\n",
        "The developer does not need to load the model to their RAM while creating a object for the model. When OpenAI API or HuggingFace Hub models are used we are actually taking a end point from their hub to run our inferences. But the default Hugging Face Hub inference APIs do not use specialized hardware and, therefore, can be slow. They are also not suitable for running larger models like bigscience/bloom-560m or google/flan-t5-xxl."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "JEM-KHplGPQq",
        "outputId": "85b19792-4598-43d8-aef7-651384993d4f"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-abb4c506226f>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# ask the user question about IPL 2023\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_output_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             outputs = (\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    139\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 output = (\n\u001b[0;32m--> 198\u001b[0;31m                     self._generate(\n\u001b[0m\u001b[1;32m    199\u001b[0m                         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             text = (\n\u001b[0;32m--> 498\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/huggingface_hub.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0m_model_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_model_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error raised by inference API: {response['error']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, params, data, raw_response)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# Make API call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Let the user handle the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \"\"\"\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'POST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    527\u001b[0m         }\n\u001b[1;32m    528\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    715\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    464\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## Initialize the environment variable for the use of HF HUB API Key\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACE_HUB_KEY')\n",
        "\n",
        "from langchain import HuggingFaceHub, LLMChain\n",
        "\n",
        "# initialize Hub LLM\n",
        "hub_llm = HuggingFaceHub(\n",
        "        repo_id='google/flan-t5-xl',\n",
        "        model_kwargs={'temperature':1e-10}\n",
        ")\n",
        "\n",
        "# create prompt template > LLM chain\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=hub_llm\n",
        ")\n",
        "\n",
        "# ask the user question about IPL 2023\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5DU3Va1mtvI"
      },
      "source": [
        "##### Option 2 : OpenAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7CEzn8zSz6U",
        "outputId": "7122ff8d-98f6-460a-c312-5a0ed923e3e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The 2011 Cricket World Cup was won by India.\n"
          ]
        }
      ],
      "source": [
        "## Initialize the environment variable for the use of OpenAI API Key\n",
        "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_KEY')\n",
        "\n",
        "from langchain import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "davinci = OpenAI(model_name='text-davinci-003',\n",
        "                 temperature = 0.1,\n",
        "                 max_tokens = 256)\n",
        "\n",
        "# create prompt template > LLM chain\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "# ask the user question about WC 2011\n",
        "question = \"Which team won the 2011 Cricket world cup\"\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLFaLq7fsO6D"
      },
      "source": [
        "## Examples of Langchain Use Case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TEHJHW8sUV9"
      },
      "source": [
        "### 1. Query DataFrame / CSV : With Zero Shot Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "0EHSAVacgQn9",
        "outputId": "0a66b1d7-8e1e-451f-92c4-4f3c59d4ac4a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7ede1123-f13c-47cc-b621-d9449467117b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total_bill</th>\n",
              "      <th>tip</th>\n",
              "      <th>sex</th>\n",
              "      <th>smoker</th>\n",
              "      <th>day</th>\n",
              "      <th>time</th>\n",
              "      <th>size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16.99</td>\n",
              "      <td>1.01</td>\n",
              "      <td>Female</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.34</td>\n",
              "      <td>1.66</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21.01</td>\n",
              "      <td>3.50</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ede1123-f13c-47cc-b621-d9449467117b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7ede1123-f13c-47cc-b621-d9449467117b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7ede1123-f13c-47cc-b621-d9449467117b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   total_bill   tip     sex smoker  day    time  size\n",
              "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
              "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
              "2       21.01  3.50    Male     No  Sun  Dinner     3"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=sns.load_dataset(\"tips\")[:20]  ## working with only 50 rows as of now due to OpenAI resource constraint\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98SCHaTauEVJ"
      },
      "source": [
        "In LangChain agent is something that lets us establish the connection with the outside world. For example for humans to perform mathematical coputation, calculator is the agent, for information retrieval google is the agent. <br>\n",
        "Similarly, for LLMs to interact with the outside world, there are agents. For setting up connection with the pandas DataFrame create_pandas_dataframe_agent is there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI34GIPYsbWx"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import create_pandas_dataframe_agent , initialize_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or0lfpaesbTK",
        "outputId": "bebd95a6-9d3c-4d5e-ab9b-1ed5fc2ef949"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "langchain.agents.agent.AgentExecutor"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## return_intermediate_steps=True will let you access return the intermediate steps too\n",
        "df_agent =  create_pandas_dataframe_agent(OpenAI(model='text-davinci-002',\n",
        "                                                 temperature =0), df, verbose =True,\n",
        "                                                 return_intermediate_steps=True)\n",
        "type(df_agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64RmWbkmsbRD",
        "outputId": "f51573b5-508f-4c10-ced2-1828aa29c952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to count the number of rows \n",
            "Action: python_repl_ast\n",
            "Action Input: len(df)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m20\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer \n",
            "Final Answer: 20\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'How many rows are there ?',\n",
              " 'output': '20',\n",
              " 'intermediate_steps': [(AgentAction(tool='python_repl_ast', tool_input='len(df)', log='Thought: I need to count the number of rows \\nAction: python_repl_ast\\nAction Input: len(df)'),\n",
              "   20)]}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Chain of thoughts\n",
        "agent_response  = df_agent('How many rows are there ?')\n",
        "agent_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E5xeKwAy7Wx",
        "outputId": "bcb3c4b6-e1a2-4387-cae3-5a325ce530f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output of the chain :  20\n"
          ]
        }
      ],
      "source": [
        "## Output of the LLM\n",
        "output = agent_response['output']\n",
        "print(\"Output of the chain : \", output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpmtEZJpzqsG",
        "outputId": "cea5c9ee-704c-4cc1-8a47-ee2ff58ff517"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AgentAction(tool='python_repl_ast', tool_input='len(df)', log='Thought: I need to count the number of rows \\nAction: python_repl_ast\\nAction Input: len(df)')"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Final thought of the LLM while producing the result\n",
        "response = agent_response['intermediate_steps'][0][0]\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "n6ZVznsnsbMA",
        "outputId": "5beb6de3-91d6-4dca-937d-78de99c00a2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I should group the data by sex and day\n",
            "Action: python_repl_ast\n",
            "Action Input: df.groupby(['sex', 'day']).mean()\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m             total_bill       tip      size\n",
            "sex    day                                 \n",
            "Male   Thur         NaN       NaN       NaN\n",
            "       Fri          NaN       NaN       NaN\n",
            "       Sat    20.650000  3.350000  3.000000\n",
            "       Sun    17.521538  2.876923  2.692308\n",
            "Female Thur         NaN       NaN       NaN\n",
            "       Fri          NaN       NaN       NaN\n",
            "       Sat          NaN       NaN       NaN\n",
            "       Sun    19.828333  2.968333  3.000000\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<string>:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m I should look at the data for only Sunday\n",
            "Action: python_repl_ast\n",
            "Action Input: df.groupby(['sex', 'day']).mean().loc['Female', 'Sun']\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mtotal_bill    19.828333\n",
            "tip            2.968333\n",
            "size           3.000000\n",
            "Name: (Female, Sun), dtype: float64\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<string>:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m I should look at the data for only Sunday\n",
            "Action: python_repl_ast\n",
            "Action Input: df.groupby(['sex', 'day']).mean().loc['Male', 'Sun']\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mtotal_bill    17.521538\n",
            "tip            2.876923\n",
            "size           2.692308\n",
            "Name: (Male, Sun), dtype: float64\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<string>:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The mean of tips given by male on Sunday is 2.876923 and the mean of tips given by female on Sunday is 2.968333\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The mean of tips given by male on Sunday is 2.876923 and the mean of tips given by female on Sunday is 2.968333'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## return_intermediate_steps=False will return only the final output\n",
        "df_agent =  create_pandas_dataframe_agent(OpenAI(model='text-davinci-002',\n",
        "                                                 temperature =0), df, verbose =True,\n",
        "                                                 return_intermediate_steps=False)\n",
        "\n",
        "## Chain of thoughts\n",
        "df_agent.run('Find the mean of tips given by male and female on sunday')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inBa3P-5_fYg",
        "outputId": "f8f35ee3-2e3b-4433-b890-422795bf2717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I should group the data by sex and day and then find the mean of the tips\n",
            "Action: python_repl_ast\n",
            "Action Input: df.groupby(['sex', 'day']).mean()['tip']\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3msex     day \n",
            "Male    Thur         NaN\n",
            "        Fri          NaN\n",
            "        Sat     3.350000\n",
            "        Sun     2.876923\n",
            "Female  Thur         NaN\n",
            "        Fri          NaN\n",
            "        Sat          NaN\n",
            "        Sun     2.968333\n",
            "Name: tip, dtype: float64\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<string>:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m I should fill in the missing values\n",
            "Action: python_repl_ast\n",
            "Action Input: df.groupby(['sex', 'day']).mean()['tip'].fillna(0)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3msex     day \n",
            "Male    Thur    0.000000\n",
            "        Fri     0.000000\n",
            "        Sat     3.350000\n",
            "        Sun     2.876923\n",
            "Female  Thur    0.000000\n",
            "        Fri     0.000000\n",
            "        Sat     0.000000\n",
            "        Sun     2.968333\n",
            "Name: tip, dtype: float64\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<string>:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The mean of tips given by male and female on sunday and saturday is 2.876923 for males and 2.968333 for females\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Find the mean of tips given by male and female on sunday and saturday',\n",
              " 'output': 'The mean of tips given by male and female on sunday and saturday is 2.876923 for males and 2.968333 for females',\n",
              " 'intermediate_steps': [(AgentAction(tool='python_repl_ast', tool_input=\"df.groupby(['sex', 'day']).mean()['tip']\", log=\"Thought: I should group the data by sex and day and then find the mean of the tips\\nAction: python_repl_ast\\nAction Input: df.groupby(['sex', 'day']).mean()['tip']\"),\n",
              "   sex     day \n",
              "   Male    Thur         NaN\n",
              "           Fri          NaN\n",
              "           Sat     3.350000\n",
              "           Sun     2.876923\n",
              "   Female  Thur         NaN\n",
              "           Fri          NaN\n",
              "           Sat          NaN\n",
              "           Sun     2.968333\n",
              "   Name: tip, dtype: float64),\n",
              "  (AgentAction(tool='python_repl_ast', tool_input=\"df.groupby(['sex', 'day']).mean()['tip'].fillna(0)\", log=\" I should fill in the missing values\\nAction: python_repl_ast\\nAction Input: df.groupby(['sex', 'day']).mean()['tip'].fillna(0)\"),\n",
              "   sex     day \n",
              "   Male    Thur    0.000000\n",
              "           Fri     0.000000\n",
              "           Sat     3.350000\n",
              "           Sun     2.876923\n",
              "   Female  Thur    0.000000\n",
              "           Fri     0.000000\n",
              "           Sat     0.000000\n",
              "           Sun     2.968333\n",
              "   Name: tip, dtype: float64)]}"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## return_intermediate_steps=False will return only the final output\n",
        "df_agent =  create_pandas_dataframe_agent(OpenAI(model='text-davinci-002',\n",
        "                                                 temperature =0), df, verbose =True,\n",
        "                                                 return_intermediate_steps=True)\n",
        "\n",
        "## Chain of thoughts\n",
        "agent_response  = df_agent('Find the mean of tips given by male and female on sunday and saturday')\n",
        "agent_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VBsRF3bK_mKi",
        "outputId": "67f86673-9e24-4fbf-deb6-cd292434a95a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"df.groupby(['sex', 'day']).mean()['tip'].fillna(0)\""
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### After all the thought process this is the end query that the LLM run to get the result\n",
        "agent_response['intermediate_steps'][-1][0].log.split('\\n')[-1].split('Action Input:')[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kyrJYcrgAWnO",
        "outputId": "dfb42317-ec96-417a-de3c-79d81ac9a86d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The mean of tips given by male and female on sunday and saturday is 2.876923 for males and 2.968333 for females'"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#### Output\n",
        "agent_response['output']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cxHqfIYsavB"
      },
      "outputs": [],
      "source": [
        "# dumps = json.dumps(response.__dict__)\n",
        "# loads = json.loads(dumps)\n",
        "# loads['log'].split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vvS8vyXC-KO"
      },
      "source": [
        "### 2. Query DataFrame / CSV : With Few Shots Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "244r0nOKBxcB"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(query):\n",
        "    \"\"\"\n",
        "    Query an agent and return the response as a string.\n",
        "\n",
        "    Args:\n",
        "        agent: The agent to query.\n",
        "        query: The query to ask the agent.\n",
        "\n",
        "    Returns:\n",
        "        The response from the agent as a string.\n",
        "    \"\"\"\n",
        "    # Prepare the prompt with query guidelines and formatting\n",
        "    prompt = (\n",
        "        \"\"\"\n",
        "        Let's decode the way to respond to the query asked within the delimiter #. The responses depend on the type of information requested in the query.\n",
        "\n",
        "        1. If the query requires a table, format your answer like this:\n",
        "           {\"table\": {\"columns\": [\"column1\", \"column2\", ...], \"data\": [[value1, value2, ...], [value1, value2, ...], ...]}}\n",
        "\n",
        "        2. For a bar chart, respond like this:\n",
        "           {\"bar\": {\"columns\": [\"A\", \"B\", \"C\", ...], \"data\": [25, 24, 10, ...]}}\n",
        "\n",
        "        3. If a line chart is more appropriate, your reply should look like this:\n",
        "           {\"line\": {\"columns\": [\"A\", \"B\", \"C\", ...], \"data\": [25, 24, 10, ...]}}\n",
        "\n",
        "        Note: We only accommodate two types of charts: \"bar\" and \"line\".\n",
        "\n",
        "        4. For a plain question that doesn't need a chart or table, your response should be:\n",
        "           {\"answer\": \"Your answer goes here\"}\n",
        "\n",
        "        For example:\n",
        "           {\"answer\": \"The Product with the highest Orders is '15143Exfo'\"}\n",
        "\n",
        "        5. If the answer is not known or available, respond with:\n",
        "           {\"answer\": \"I do not know.\"}\n",
        "\n",
        "        Return all output as a string. Remember to encase all strings in the \"columns\" list and data list in double quotes.\n",
        "        For example: {\"columns\": [\"Products\", \"Orders\"], \"data\": [[\"51993Masc\", 191], [\"49631Foun\", 152]]}\n",
        "\n",
        "        Now, let's tackle the query step by step. Here's the query for you to work on:\n",
        "        \"\"\"\n",
        "        + '#'+ query +'#'\n",
        "    )\n",
        "\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvpsQDRmNNkv"
      },
      "outputs": [],
      "source": [
        "def create_plot(response_dict: dict):\n",
        "\n",
        "    # Check if the response is an answer.\n",
        "    if \"answer\" in response_dict:\n",
        "        return response_dict[\"answer\"]\n",
        "\n",
        "    # Check if the response is a bar chart.\n",
        "    if \"bar\" in response_dict:\n",
        "        data = response_dict[\"bar\"]\n",
        "        try:\n",
        "            plt.bar(data['columns'], data['data'])\n",
        "            return plt.show()\n",
        "        except ValueError:\n",
        "            print(f\"Couldn't create DataFrame from data: {data}\")\n",
        "\n",
        "    # Check if the response is a line chart.\n",
        "    if \"line\" in response_dict:\n",
        "        data = response_dict[\"line\"]\n",
        "        try:\n",
        "            plt.line(data['columns'], data['data'])\n",
        "            return plt.show()\n",
        "        except ValueError:\n",
        "            print(f\"Couldn't create DataFrame from data: {data}\")\n",
        "\n",
        "\n",
        "    # Check if the response is a table.\n",
        "    if \"table\" in response_dict:\n",
        "        data = response_dict[\"table\"]\n",
        "        df = pd.DataFrame(data[\"data\"], columns=data[\"columns\"])\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQNzGmH8EtHL"
      },
      "outputs": [],
      "source": [
        "df_agent =  create_pandas_dataframe_agent(OpenAI(model='text-davinci-002',\n",
        "                                                 temperature =0), df, verbose =True,\n",
        "                                                 return_intermediate_steps=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCkQ1rNnBxYo",
        "outputId": "b91e16f5-1ff4-4f9f-a4e7-36f57d047259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to create a bar chart that shows the total tips given by male and female\n",
            "Action: python_repl_ast\n",
            "Action Input: df.groupby('sex').sum()['tip']\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3msex\n",
            "Male      40.75\n",
            "Female    17.81\n",
            "Name: tip, dtype: float64\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<string>:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: {\"bar\": {\"columns\": [\"Male\", \"Female\"], \"data\": [40.75, 17.81]}}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': '\\n        Let\\'s decode the way to respond to the query asked within the delimiter #. The responses depend on the type of information requested in the query. \\n\\n        1. If the query requires a table, format your answer like this:\\n           {\"table\": {\"columns\": [\"column1\", \"column2\", ...], \"data\": [[value1, value2, ...], [value1, value2, ...], ...]}}\\n\\n        2. For a bar chart, respond like this:\\n           {\"bar\": {\"columns\": [\"A\", \"B\", \"C\", ...], \"data\": [25, 24, 10, ...]}}\\n\\n        3. If a line chart is more appropriate, your reply should look like this:\\n           {\"line\": {\"columns\": [\"A\", \"B\", \"C\", ...], \"data\": [25, 24, 10, ...]}}\\n\\n        Note: We only accommodate two types of charts: \"bar\" and \"line\".\\n\\n        4. For a plain question that doesn\\'t need a chart or table, your response should be:\\n           {\"answer\": \"Your answer goes here\"}\\n\\n        For example:\\n           {\"answer\": \"The Product with the highest Orders is \\'15143Exfo\\'\"}\\n\\n        5. If the answer is not known or available, respond with:\\n           {\"answer\": \"I do not know.\"}\\n\\n        Return all output as a string. Remember to encase all strings in the \"columns\" list and data list in double quotes. \\n        For example: {\"columns\": [\"Products\", \"Orders\"], \"data\": [[\"51993Masc\", 191], [\"49631Foun\", 152]]}\\n\\n        Now, let\\'s tackle the query step by step. Here\\'s the query for you to work on: \\n        #create a bar chart showing the totat tips given by male and female#',\n",
              " 'output': '{\"bar\": {\"columns\": [\"Male\", \"Female\"], \"data\": [40.75, 17.81]}}',\n",
              " 'intermediate_steps': [(AgentAction(tool='python_repl_ast', tool_input=\"df.groupby('sex').sum()['tip']\", log=\"Thought: I need to create a bar chart that shows the total tips given by male and female\\nAction: python_repl_ast\\nAction Input: df.groupby('sex').sum()['tip']\"),\n",
              "   sex\n",
              "   Male      40.75\n",
              "   Female    17.81\n",
              "   Name: tip, dtype: float64)]}"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run the prompt through the agent and capture the response.\n",
        "query = 'create a bar chart showing the totat tips given by male and female'\n",
        "prompt = generate_prompt(query)\n",
        "response = df_agent(prompt)\n",
        "\n",
        "# Return the response converted to a string.\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmP9M-GqBxV8",
        "outputId": "1ffb6203-a1e1-4106-ca48-f580684aa26b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bar': {'columns': ['Male', 'Female'], 'data': [40.75, 17.81]}}"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Output : It returns in string format but use json.loads to load in dict format\n",
        "output_string  = response['output']\n",
        "output_dict = json.loads(output_string)\n",
        "output_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eeDdC5eUBxTA",
        "outputId": "1d923ce7-89a9-4671-c7a8-2a9d59cd99f1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"df.groupby('sex').sum()['tip']\""
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Final thought of the chain while producing the result\n",
        "response['intermediate_steps'][-1][0].log.split('\\n')[-1].split('Action Input:')[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "xwjVOeJQNQpQ",
        "outputId": "89357990-4018-4959-bcc2-b4219561208d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#### Creating plot from the response given by the OpenAI response\n",
        "create_plot(output_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvLak8ZTBxN7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gilpZTUArUp0"
      },
      "source": [
        "Things to read 🇰\n",
        "1. Few shots learning : https://www.youtube.com/watch?v=J_0qvRt4LNk&ab_channel=SamWitteveen\n",
        "2. Plot Generation : https://levelup.gitconnected.com/talk-to-your-csv-how-to-visualize-your-data-with-langchain-and-streamlit-5cb8a0db87e0"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
